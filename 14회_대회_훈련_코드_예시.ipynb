{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dacon  14회 KB 금융문자 분석 모델링 경진대회\n",
    "## dacon (팀명)\n",
    "## 2020년 12월 24일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링 코드 작성방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 코드 관련\n",
    "\n",
    "1) 입상자는 코드 제출 필수. 제출 코드는 예측 결과를 리더보드 점수로 복원할 수 있어야 함\n",
    "\n",
    "2) 코드 제출시 확장자가 R user는 R or .rmd. Python user는 .py or .ipynb\n",
    "\n",
    "3) 코드에 ‘/data’ 데이터 입/출력 경로 포함 제출 or R의 경우 setwd(\" \"), python의 경우 os.chdir을 활용하여 경로 통일\n",
    "\n",
    "4) 전체 프로세스를 일목요연하게 정리하여 주석을 포함하여 하나의 파일로 제출\n",
    "\n",
    "5) 모든 코드는 오류 없이 실행되어야 함(라이브러리 로딩 코드 포함되어야 함).\n",
    "\n",
    "6) 코드와 주석의 인코딩은 모두 UTF-8을 사용하여야 함\n",
    "\n",
    " \n",
    "B 외부 데이터 관련\n",
    "\n",
    "1) 외부 공공 데이터 (날씨 정보 등) 사용이 가능하나, 코드 제출 시 함께 제출\n",
    "\n",
    "2) 공공 데이터 외의 외부 데이터는 법적인 제약이 없는 경우에만 사용 가능\n",
    "\n",
    "3) 외부 데이터를 크롤링할 경우, 크롤링 코드도 함께 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import os\n",
    "os.chdir('../0_Data')\n",
    "\n",
    "from collections import namedtuple, Counter\n",
    "from tqdm import tqdm\n",
    "from gensim.models import doc2vec\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 데이터 로드\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "test_data=pd.read_csv(\"public_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Code(크롤링 진행 시 기입)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2019) #반복 수행시에도 동일한 결과 나올 수 있도록 시드 번호 지정\n",
    "train_nsm_list=list(train[train['smishing']!=1].index)\n",
    "train_nsmishing=random.sample(train_nsm_list, 11750 )\n",
    "\n",
    "random.seed(2019)\n",
    "train_sm_list=list(train[train['smishing']==1].index)\n",
    "train_smishing=random.sample(train_sm_list, 850 ) #0.066과 제일 비슷하게 나올 수 있도록  train data under sampling\n",
    "\n",
    "train_data=train.iloc[train_smishing+train_nsmishing,:].reset_index(drop=True) #under sampling으로 나온 index들로 train data 선별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12600/12600 [01:53<00:00, 111.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1626/1626 [00:21<00:00, 74.71it/s]\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "removeword=['[Web발신]', \"(광고)\", \"\\n\", \"\\r\",'X','XX','XXX','XXXX','XXXXX','XXXXXX','XXXXXXX',\n",
    "            '.', '을', '를', '이', '가', '-', '(', ')', ':', '!', '?', ')-', '.-', 'ㅡ','..', '.(', '은', '는'] #그냥 사용시 토큰으로 같이 인식됨\n",
    "# X를 추가한 이유 : 제거 안하니까 kbXXX이런식으로 같이 인식해버림, 굳이 그렇게 인식 될 이유가 전혀 없어보였음\n",
    "\n",
    "def ishangul(text):\n",
    "    \"\"\"해당 글자가 한글인지 찾아주는 함수입니다.\"\"\"\n",
    "    hanCount = len(re.findall(u'[\\u3130-\\u318F\\uAC00-\\uD7A3]+', text))\n",
    "    return hanCount > 0\n",
    "\n",
    "def blank(text):\n",
    "    \"\"\"한글과 한글이 아닌 단어사이에 공백을 삽입합니다.\"\"\"\n",
    "    return_doc = []\n",
    "    pre_hangul = True\n",
    "    for i in text:\n",
    "        hangul = ishangul(i)\n",
    "        if pre_hangul != hangul:\n",
    "            return_doc.append(\" \")\n",
    "            return_doc.append(i)\n",
    "        else:\n",
    "            return_doc.append(i)\n",
    "        pre_hangul = hangul\n",
    "    return \"\".join(return_doc)\n",
    "\n",
    "def tokenize(doc, norm=True, stem=True, removeword=None):\n",
    "    \"\"\"\n",
    "    공백 삽입을 찾아내고, 이후 doc2vec 을 하기위한 데이터로 바꾸줍니다.\n",
    "    특수문자를 'Punctuation' 으로 바꿉니다.\n",
    "    외국어와 숫자를 제거합니다.\n",
    "    \"\"\"\n",
    "    if removeword:\n",
    "        for word in removeword:\n",
    "            doc = doc.replace(word, \" \") #없앨 단어 대체\n",
    "    toekns = [\"/\".join(word) for word in okt.pos(doc)] #토큰 화\n",
    "    doc = blank(doc)  # 해당 문자에서 영어와 한글 사이에 공백을 넣는 함수\n",
    "    \n",
    "    adj_toekns = []\n",
    "    for t in toekns:\n",
    "        if \"Punctuation\" in t:\n",
    "            adj_toekns.append(\"Punctuation\")\n",
    "        elif \"Foreign\" in t: # 외국어 제거\n",
    "            pass\n",
    "        elif \"Number\" in t:  # 숫자 제거\n",
    "            pass\n",
    "        else:\n",
    "            adj_toekns.append(t) \n",
    "    return adj_toekns #외국어, 숫자 제거하고 특수문자는 Punctuation 자체로 바꾸고, url도 해당 url 특징에 맞게 바꾸는 함수\n",
    "\n",
    "\n",
    "#위에 주어진 함수를돌리기 위해서 array화 실행\n",
    "train_data_list = np.array(train_data[[\"text\", \"smishing\"]].values.tolist())\n",
    "\n",
    "train_docs = [ (tokenize( row[0], removeword=removeword ), row[1] ) for row in tqdm(train_data_list)]\n",
    "\n",
    "test_data['smishing']=2\n",
    "\n",
    "test_data_list = np.array(test_data[[\"text\",\"smishing\"]].values.tolist())\n",
    "\n",
    "test_docs = [ ( tokenize(row[0],  removeword=removeword ), row[1] ) for row in tqdm(test_data_list) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "\n",
    "tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs] # d는 단어랑 타입, c는 스미싱 여부 라벨링\n",
    "tagged_test_docs = [TaggedDocument(d, [c]) for d, c in test_docs] # d는 단어랑 타입, c는 스미싱 여부 라벨링(test에서는 없으니 임의로 설정된 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 탐색적 자료분석\n",
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../1_Model')\n",
    "\n",
    "doc_init = doc2vec.Doc2Vec(size=1000, window=10, min_count=5, workers=11,alpha=0.025, min_alpha=0.025, iter=2, seed = 2019) \n",
    "doc_init.build_vocab(tagged_train_docs) \n",
    "\n",
    "doc_init.train(tagged_train_docs, epochs=doc_init.iter, total_examples=doc_init.corpus_count)\n",
    "\n",
    "doc_init.save('seven.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:03:53.428396Z",
     "start_time": "2019-12-06T05:03:53.422391Z"
    }
   },
   "outputs": [],
   "source": [
    "def t1(tt):\n",
    "    doc_init.random.seed(0) #동일한 시드를 놔둬서 같은 array의 값이 나오게 설정해둠\n",
    "    return doc_init.infer_vector(tt.words) #단어를 vector화 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:05:11.193726Z",
     "start_time": "2019-12-06T05:03:53.431392Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x=[ t1(i) for i in tagged_train_docs ]\n",
    "\n",
    "test_x=[ t1(i) for i in tagged_test_docs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:05:38.116513Z",
     "start_time": "2019-12-06T05:05:11.206728Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#스미싱 링크 라벨\n",
    "\n",
    "train_select = train_data[\"smishing\"]# 스미싱 링크 여부랑 스미싱 여부만 가져오기\n",
    "\n",
    "train_embedding = pd.DataFrame(train_x) #vector to dataframe\n",
    "train_tf = pd.concat([train_embedding, train_select], axis=1)\n",
    "\n",
    "test_tf = pd.DataFrame(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:05:38.610515Z",
     "start_time": "2019-12-06T05:05:38.162516Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data2=train_tf.copy()\n",
    "test_data2=test_tf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:06:22.756466Z",
     "start_time": "2019-12-06T05:06:22.752465Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:05:40.553746Z",
     "start_time": "2019-12-06T05:05:40.528748Z"
    }
   },
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(n_estimators=500, learning_rate=0.025,\n",
    "\n",
    "                     max_depth=7,\n",
    "\n",
    "                     min_child_samples=100,\n",
    "\n",
    "                     random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:05:40.709747Z",
     "start_time": "2019-12-06T05:05:40.556750Z"
    }
   },
   "outputs": [],
   "source": [
    "train_xx=train_data2.copy()\n",
    "train_y = train_xx['smishing']\n",
    "del train_xx['smishing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:06:10.048554Z",
     "start_time": "2019-12-06T05:05:40.725746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.025, max_depth=7,\n",
       "               min_child_samples=100, min_child_weight=0.001,\n",
       "               min_split_gain=0.0, n_estimators=500, n_jobs=-1, num_leaves=31,\n",
       "               objective=None, random_state=1234, reg_alpha=0.0, reg_lambda=0.0,\n",
       "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.fit(train_xx, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:06:29.476464Z",
     "start_time": "2019-12-06T05:06:27.928465Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_ten=lgbm.predict_proba(test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:06:29.513468Z",
     "start_time": "2019-12-06T05:06:29.480465Z"
    }
   },
   "outputs": [],
   "source": [
    "proba = [ i[1] for i in y_pred_ten ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T05:06:56.024550Z",
     "start_time": "2019-12-06T05:06:55.903551Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data['pred']=proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgb.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm, 'lgb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생략"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
