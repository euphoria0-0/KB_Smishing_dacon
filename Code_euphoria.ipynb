{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Code_euphoria.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uU8TE6MvgKLJ"
      },
      "source": [
        "## Dacon 14회 KB 금융문자 분석 모델링 경진대회\n",
        "### euphoria , 2020.01.17\n",
        "### Summary\n",
        "1. Text Cleaning and make Bi-gram\n",
        "2. Data Sampling\n",
        "3. Text Preprocessing with tf.keras.Tokenizer and pad_sequences\n",
        "4. train Bi-Directional LSTM model\n",
        "5. Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnBsm5b820wD"
      },
      "source": [
        "## 1. 라이브러리 및 데이터\n",
        "### 1-1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kOY7wsx_LKCj",
        "colab": {}
      },
      "source": [
        "# data preprocessing\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "# text tokenizing\n",
        "import re\n",
        "from konlpy.tag import Mecab\n",
        "# modeling\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, SpatialDropout1D, Dropout\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Ap8IpUX5CYJ"
      },
      "source": [
        "### 1-2. Settings for reproducible results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2unTANeu49W0",
        "outputId": "fc7efb49-7cc3-4744-d617-4e864eec8338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sd = random.randint(0,99999999)\n",
        "print(sd)\n",
        "\n",
        "np.random.seed(sd)\n",
        "random.seed(sd)\n",
        "os.environ['PYTHONHASHSEED']=str(sd)\n",
        "\n",
        "config = tf.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
        "tf.set_random_seed(sd)\n",
        "\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=config)\n",
        "K.set_session(sess)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73717145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y7vMYQkt7r8m"
      },
      "source": [
        "### 1-3. Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2arA7qSIgtGt",
        "outputId": "5c9f1a2a-9bce-4085-95a3-84c4c6a6deca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#os.chdir('/content/gdrive/My Drive/Colab Notebooks/smishing/') \n",
        "train = pd.read_csv('0_Data/train.csv')\n",
        "print(train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(295945, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GR6rYpgo3ubq"
      },
      "source": [
        "## 2. 데이터 전처리\n",
        "### 2-1. Data Cleaning\n",
        "- Mecab을 이용하여 텍스트를 정제합니다.\n",
        "    - 영문은 소문자화하고 한글, 영문 제외 모두 제거하였습니다.\n",
        "    - 영문 중 xxx 등으로 비식별처리된 것은 모두 제거하였습니다.\n",
        "- Bi-gram을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iDG7diik76t8",
        "colab": {}
      },
      "source": [
        "def text_preprocessing(text_list):\n",
        "    '''\n",
        "    args: text list that wants to tokenize\n",
        "    return: bigram list\n",
        "    '''\n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] # 제거: 불용어, 한글 영문 외 문자, XXX 등 비식별처리된 문자\n",
        "    tokenizer = Mecab()\n",
        "    bigram_list = []\n",
        "    for text in text_list:\n",
        "        txt = re.sub('[^가-힣a-z]',' ',text.lower())\n",
        "        txt = re.sub('x{1,}',' ',txt)\n",
        "        token = tokenizer.morphs(txt)  # tokenizing\n",
        "        token = [t for t in token if t not in stopwords or type(t)!= float] # text cleaning\n",
        "        bigram = [token[i]+'.'+token[i+1] for i in range(len(token)-1)]  # bi-gram\n",
        "        bigram_list.append(' '.join(bigram))\n",
        "    return bigram_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4UEkLnCO7Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['bigram'] = text_preprocessing(train.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jBqAVONG-PBo"
      },
      "source": [
        "### 2-2. Data Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m-gbA3iJ0neA",
        "outputId": "ad8ae86a-8da5-45e2-f588-81d0334bc92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from collections import Counter\n",
        "Counter(train['smishing'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 277242, 1: 18703})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bdFKHQqh0va_"
      },
      "source": [
        "- 6.31975535994864%의 비율만이 스미싱 문자임을 알 수 있습니다.\n",
        "- 즉, 데이터 불균형 문제(Data Imbalance Problem)가 발생하였습니다.\n",
        "- 따라서 데이터를 혼합하여 샘플링하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "itgYwhhGBzzA",
        "colab": {}
      },
      "source": [
        "def train_data_sampling(train, seed=1234, a=3, b=3):\n",
        "    '''\n",
        "    Args: train data, seed number, a(under sampling), b(over sampling)\n",
        "    Return: sampling index\n",
        "    '''\n",
        "    train_nsm_idx=list(train[train['smishing']==0].index)\n",
        "    train_sm_idx=list(train[train['smishing']==1].index)\n",
        "    random.seed(seed)\n",
        "    train_nsm_idx = random.sample(train_nsm_idx, k=18703*a)\n",
        "    random.seed(seed)\n",
        "    train_sm_idx = random.choices(train_sm_idx, k=18703*b)\n",
        "    train_idx = train_nsm_idx + train_sm_idx\n",
        "    print(train_idx[:5])\n",
        "    random.shuffle(train_idx)\n",
        "    print(train_idx[:5])\n",
        "    return train_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kvFIOD9HBY3T",
        "outputId": "af5c6237-dd30-4a75-a28f-40408d1483c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "trn_idx = train_data_sampling(train, seed=sd, a=3, b=2)\n",
        "df_train = train.iloc[trn_idx].reset_index(drop=True)\n",
        "print(df_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[205785, 278157, 281730, 110774, 4960]\n",
            "[46358, 251831, 6065, 255550, 222758]\n",
            "(93515, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NNLLwnt1bCm-"
      },
      "source": [
        "### 2-3. text pre-processing\n",
        "- 모델을 학습하기 위해 텍스트 데이터를 시퀀스 데이터로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9pYUYKgCEpj8",
        "colab": {}
      },
      "source": [
        "def save_tokenizer(tokenizer, mname):\n",
        "    with open('1_Model/'+mname+'.pickle', 'wb') as f:\n",
        "        pickle.dump(tokenizer, f, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VtzQUkXSB4oC",
        "colab": {}
      },
      "source": [
        "def text2sequence(train_text, max_len=1000):\n",
        "    '''\n",
        "    transform text to sequence\n",
        "    Args: text of train data, max length(for word embedding)\n",
        "    Return: train data(for modeling), vocabulary size\n",
        "    '''\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_text)\n",
        "    save_tokenizer(tokenizer, 'tokenizer') # save trained tokenizer\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('vocab size: ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen=max_len)\n",
        "    return X_train, vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XuGsuVebewK3",
        "outputId": "0bebd664-068e-4103-8337-6c1cb10eb0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_y = df_train['smishing']\n",
        "train_X, vocab_size = text2sequence(df_train['bigram'], max_len=341)\n",
        "print(train_X.shape, train_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  22912\n",
            "(93515, 341) (93515,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ua9JfvszX407"
      },
      "source": [
        "## 3. 탐색적 자료 분석 (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "onDiYpU31eJ5"
      },
      "source": [
        "- 위의 max_len을 정하기 위해 EDA를 하여 적당한 len을 결정하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "649cbab8-a4e5-43e6-e209-d523f8d30949",
        "id": "OTyqu2WfTrzs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "pd.Series([len(x.split()) for x in df_train['bigram']]).describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    93515.000000\n",
              "mean       162.464931\n",
              "std        156.272046\n",
              "min          0.000000\n",
              "25%         18.000000\n",
              "50%         89.000000\n",
              "75%        341.000000\n",
              "max        528.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gNsXkyJMhmhB"
      },
      "source": [
        "## 4. 변수 선택 및 모델 구축\n",
        "### Build Model\n",
        "- 모델 평가 지표는 AUC입니다.\n",
        "- 모델은 Bi-LSTM (Bi-Directional Long Short Term Memory)을 사용하였습니다.\n",
        "- 텍스트는 Bi-gram을 sequence로 변환한 데이터로 학습하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JY49FyLN39Lm",
        "colab": {}
      },
      "source": [
        "auc_ = 0\n",
        "def auc_score(y_true, y_pred):\n",
        "    global auc_\n",
        "    try:\n",
        "        auc_ = roc_auc_score( y_true, y_pred, average='macro', sample_weight = None).astype('float32')\n",
        "    except ValueError:\n",
        "        pass\n",
        "    return auc_\n",
        "\n",
        "def auc(y_true, y_pred):\n",
        "    score = tf.py_func( lambda y_true, y_pred : auc_score(y_true, y_pred) , [y_true, y_pred], 'float32', stateful = False, name = 'sklearnAUC' )\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_kCh0a2eLFxj",
        "colab": {}
      },
      "source": [
        "def BiLSTM(vocab_size, max_len=1000):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 128, input_length = max_len))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='tanh', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[auc])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpJnBvgClEhA",
        "colab": {}
      },
      "source": [
        "def model_save(model, mname):\n",
        "    model_json = model.to_json()\n",
        "    with open('1_Model/'+mname+'.json', 'w') as json_file : \n",
        "        json_file.write(model_json)\n",
        "    model.save_weights('1_Model/'+mname+'.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zqwq_3Rmh5e2"
      },
      "source": [
        "## 5. 모델 학습 및 검증\n",
        "- 모델을 학습합니다.\n",
        "- EarlyStopping으로 과도한 학습을 하지 않고 적당한 score에 도달하면 조기종료 할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GJ0bDPajnuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('START TIME: ', datetime.now().isoformat())\n",
        "model = BiLSTM(vocab_size, max_len=341)\n",
        "early_stopping = EarlyStopping(patience=3, min_delta=0.00001)\n",
        "history = model.fit(train_X, train_y, epochs=50, batch_size=128, validation_split=0.3, callbacks=[early_stopping])\n",
        "\n",
        "model_save(model, 'model') # save trained model\n",
        "print('END TIME: ', datetime.now().isoformat())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGAGjhDqzn9a"
      },
      "source": [
        "## 6. 예측\n",
        "- 위에서 훈련한 tokenizer와 model을 load하고 test set으로 predict 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5__SCgYPVPqS",
        "colab": {}
      },
      "source": [
        "def text2sequence_test(tokenizer, test_text, max_len=1000):\n",
        "    test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "    X_test = pad_sequences(test_seq, maxlen=max_len)\n",
        "    return X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nBWa9Njzyzg",
        "colab": {}
      },
      "source": [
        "def get_prediction(test_file_path):\n",
        "    '''\n",
        "    Args: String\n",
        "    Return: Pandas DataFrame    \n",
        "    '''\n",
        "    \n",
        "    '''1. load test dataset'''\n",
        "    test = pd.read_csv(test_file_path)\n",
        "    \n",
        "    '''2. load model and tokenizer'''\n",
        "    with open('1_Model/tokenizer.pickle', 'rb') as f:\n",
        "        tokenizer_test = pickle.load(f)\n",
        "    with open('1_Model/model.json', 'r') as ff:\n",
        "        json_model = ff.read()\n",
        "    model_test = model_from_json(json_model)\n",
        "    model_test.load_weights('1_Model/model.h5')\n",
        "    \n",
        "    '''3. test data preprocessing'''\n",
        "    test['bigram'] = text_preprocessing(test.text)\n",
        "    test_X = text2sequence_test(tokenizer_test, test['bigram'], max_len=341)\n",
        "    \n",
        "    model_test.compile(optimizer='adam', loss='binary_crossentropy', metrics=[auc])\n",
        "\n",
        "    '''4. prediction'''\n",
        "    y_pred = model_test.predict(test_X, batch_size=128)\n",
        "    \n",
        "    '''5. make submission'''\n",
        "    test['smishing'] = y_pred\n",
        "    submission = test[['id','smishing']]\n",
        "    #submission.to_csv('submission.csv',index=False)\n",
        "\n",
        "    return submission"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BrDeLGrtz0gn",
        "colab": {}
      },
      "source": [
        "y_pred = get_prediction('0_Data/public_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}